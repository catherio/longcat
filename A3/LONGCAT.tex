\documentclass{article}

\newenvironment{itemizedense}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{DS-GA-1008 Assignment 3}
\author{LongCat: Catherine Olsson and Long Sha and Kevin Brown}

\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{fullpage}

\begin{document} \maketitle

\begin{abstract}
This is an abstract. TODO finish me!
\end{abstract}

\section*{Part 1: Log-exponential pooling}

For part 1 of the assignment, we tuned the hyperparameters for the baseline model after implementing log-exponential pooling. With the parameters reported below, we achieved at best 51.6\% error with 50 training epochs, using 10,000 training documents per class, and 1,000 validation documents per class. 

\subsection*{Architecture and training}
We mainly focused on the following hyperparameters: dimensionality of GloVe representation for each word (inputDim), number of filters for temporal convolution (nfilter), convolutional filter size (filtsize), pooling size (poolsize), the stride for convolution filters and pooling (stride). We eventually settled on the following hyperparameters:

\begin{enumerate}
\item GLoVe vector size: 200
\item number of filters: 60
\item filter size: 10
\item pooling size: 7
\item stride: 1
\end{enumerate}

We used stochastic gradient descent to train our baseline model. We apply the learning rate at 0.1, with a momentum of 0.01. For every 10 training epochs, we reduce the learning rate by half. The batch size we apply is 128. In general, we found that increasing the dimensionality of input data helps with reducing error, i.e. increasing GloVe representation and number of convolutional filters. 

\subsection*{Implementation of log-exponential pooling} 

We also coded a Module \texttt{TemporalLogExpPooling} that performs log-exponential pooling (Boureau et al. 2010). The log-exponential pooling follows equation \ref{logexp}:

\begin{align}
u_j = \frac{1}{\beta} \log{\frac{1}{N} \sum\limits_{i=1}^{N} e^{\beta x_i}}
\label{logexp}
\end{align}

where $u$ is the module output, $x$ is the module input, $N$ is the size of pooling window, $\beta$ is a free parameter between 0 and $\inf$. When  gets close to 0, the pooling behaves more similar to an average-pooling module, whereas close to $\inf$ makes it a max-pooling module. The log-exponential pooling is a transition between average and max-pooling. We also tuned the  parameter, and found that  in the range between 10 and 100 works best. 

We used equation \ref{logexp} to compute the output $u$ based on input $x$. Each output entry is computed within a pooling window from $x$. The computation of module output is coded in function \texttt{TemporalLogExpPooling:updateOutput()}. 

Next, we aim to compute the gradient of loss with respect to the input. Using the chain rule and principles in back propagation (LeCun et al., 1998), we compute:

\begin{align}
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial u} \frac{\partial u}{\partial x}
\label{chainrule}
\end{align}

$\frac{\partial L}{\partial u}$ represents the impact that this model's output has on the loss, given the current input. It was computed by backpropagating through the downstream modules, as \texttt{gradOutput}. The other component is the relationship between the model's own inputs and outputs, $\frac{\partial u}{\partial x}$. We obtain this by differentiating equation \ref{logexp} with respect to entries in $x$. We therefore derive equation \ref{backprop}:

\begin{align}
\frac{\partial u_j}{\partial x_i} = \frac{e^{\beta x_i}}{\sum\limits_{i=1}^{N} e^{\beta x_i}}
\label{backprop}
\end{align}

We use equation \ref{backprop} to update the gradient of loss with respect to input within a pooling window. The computation of gradient input is coded in function \texttt{TemporalLogExpPooling:updateGradInput()}. Together, these functions allow the module to work with forward() and backward() functions for model prediction and model training. 

We also run a gradient checker to ensure that the module is correct. To do so, we perturb the input by a small variation in one $x_i$ entry at a time. We forward the new input through the module to compute a new output. We compute the difference between the old and new outputs, and backpropagate this difference through the module. We check if the gradient we compute equals the perturbation we first injected. 

One caveat in our current module implementation is that we use for-loops to loop over all batches and pooling windows. This regime results in slow computation during model training, and the GPU could not efficiently accelerate the computation. We think that applying matrix multiplication for the pooling operation would speed up the module computation; this could be achieved by constructing the Jacobian matrix and multiplying through the entire vector at once. We would like to work on speeding up the module computation in the future. 

\section*{Part 2: Convolutional model}

We chose to implement a convolutional model on top of pre-trained GloVe vectors as our extension for Part 2. Convolutional models improve on the bag-of-words model in the baseline, because they can incorporate word order. We chose \emph{not} to implement a recurrent model only because of the additional implementation difficulty; recurrent models can better capture long-range information.

The ``Experimentation Phase'' section below describes the experiments we performed before settling on our final model, and what motivated our choices. The ``Final Model'' section explains the final model that is behind our submission.

\subsection*{Experimentation phase}

\subsubsection*{Choosing a model}

The first model we built was a large a multi-layer network, similar to layer 2 and beyond from Zhang's paper, but made smaller for easier testing. In our first (overly ambitious) model, layer 1 was a 7-width convolutional layer with 128 filters, operating over GloVe vectors, followed by a size-3 pooling; Layers 2 through 4 were 128-filter convolutional layers (two without pooling, one with); and the final layer was fully-connected.

However, we found this many-layer network confusing to train. We could not tell what was causing difficulties -- insufficient data? too large learning rate? too much or too little momentum? insufficient regularization?

Therefore, we decided to scale back to working with a much smaller network where we could build more intuition. Although our final performance with the small network was ultimately nowhere near as impressive as it might have been if we had successfully run a more ambitious model, we ultimately learned more about the practicalities of training neural networks by working in a much smaller sandbox where we could get rapid feedback on our experiments.

Our final model, described in detail in section~\ref{finalmodel}, contains just two convolutional layers, followed by one pooling layer, before the fully-connected layer.

\subsubsection*{Architectural decisions}

Here are a few of the architectural decisions we considered, and how we selected our final parameters:
\begin{enumerate}
\item{\textbf{Dimensionality of GloVe vectors:}} We performed most of our testing with the smallest GloVe vectors, length 50. For final tuning, we faced a memory tradeoff between dimensionality and number of training samples. We increased the size of the GloVe vectors to 100, which still enabled us to load 100,000 training samples. Higher-dimensional vectors would have negatively impacted the size of the training set; we hypothesized that training set richness was more important than input dimensionality in making this choice.
\item{\textbf{Size of convolution filters:}} How many words should the first convolutional filters span? How far should the second convolutional filters reach? We chose a size of 7 for the first layer, and 3 for the second layer, as this matched the parameterization in Zhang 2015 layers 2 and 3 (assuming that Zhang layer 1 stands for the transformation from characters to words and is thus subsumed by our GloVe input). A seven-word ``memory'' also sounded intuitively sufficient as a width for the first layer, to capture semantially interesting patterns containing sentiment information.
\item{\textbf{Amount of pooling:}} We chose not to implement pooling after the first layer, because we 
\item{\textbf{Fan-out and fan-in of convolutional layers:}} Initially we set both convolutional layers to 128 filters. Increasing the fan-out of the first layer, to 256, improved our performance from 61\% error to 57\% error at the time it was implemented. We chose not to increase the size of the second layer at the time, because we did not want to increase the number of parameters too drastically; increasing the sizes of other layers would be a promising future direction.
\end{enumerate}

\subsubsection*{Training decisions}

We started out doing very small experiments, with 10,000 samples per class, to figure out an appropriate learning rate and momentum. Our initial tests, starting with a learning rate around 0.1 and a momentum around 0.1, would plateau or oscillate with a testing error in the mid-to-low 60's; increasing the momentum, and decreasing the learning rate or instating periodic learning rate cuts by 50\%, did not improve the problem very much. We were disheartened, because the baseline model had been able to perform better with this dataset size.

We then tried two decisions which brought us substantial improvement: namely, we \emph{increased the amount of training data} to 30,000 for these early tests, and we \emph{dropped the learning rate} to 0.05 or less, with learning rate cuts of 50\% every three epochs. The additional data had an immediate positive effect, not only on our performance, but also on the model's ability to learn over time and not get stuck. We infer that convolutional networks have many more parameters than simple models like our bag of words baseline, and so need more data in order to ``shine''. The learning rate decrease also enabled us to avoid oscillations. This seems to indicate that our early ``stuckness'' on the larger model as well was due to insufficient data, rather than poorly chosen training parameters.

We did not create any new synthetic data as a means of data augmentation; not only did Zhang's paper (2015) suggest that thesaurus augmentation was not necessary, but also, we did not successfully scale our model up to use all the existing testing data anyway.

In attempting to scale up to use more data, we periodically ran into memory issues, sometimes on GPU and sometimes on CPU. We discovered that may of our GPU memory woes were actually due to too much *testing* data, not too much training data, because the testing data was not being loaded in bite-sized batch chunks. Choosing to keep our test set size at 1000, even as we loaded more training data, avoided this issue.

\subsubsection*{Text processing choices}
The baseline parsing from words to GloVe vectors operated by splitting the text on spaces and dropping punctuation. We added two improvements:

\begin{itemize}
\item \textbf{Stripping \textbackslash n}: The original text contains many words where the newline character has been appended, which prevents it from being turned into a GloVe vector correctly. We strip those by removing all instances of ``\textbackslash n''. At the time when we implemented this extension, it improved our error from 58\% to 56\%.

\item \textbf{Punctuation feature}: Using the advice from Collobert et al. (2011), in the section on ``Extending to Any Discrete Features'', we chose to add an additional ``punctuation feature'' to our GloVe vectors. We split out each block of adjacent punctuation into aseparate ``word'', which we then encode as a single ``1'' in the 101st dimension regardless of the identity or length of the punctuation. The original 100-dimensional GloVe vectors are padded with a trailing zero to make space for the punctuation feature. This does unfortunately cause words like ``don't'' to be encoded as three words, but it also allows the model to respect sentence breaks. 
\end{itemize}

Unfortunately, we accidentally forgot to include our clever punctuation improvement in our final fully-trained model.

In terms of how to handle variable-length input, we chose the approach of cutting off all the text at a certain point, and filling missing text with zeros. Initially we chose roughly 200 words (which only cut off about 10\% of the reviews), but dropped it to length 100 (actually 104, so our), on the theory that the large swath of zeros in most entries might have been causing some of our problems.

Other extensions we would have liked to try out, but did not have time for, included: adding START and END padding to the start and end of the review; creating a ``number'' feature, encoding the value of any numbers; creating a capitalization feature; backwards quantization of; and discarding rare words (to prevent misidentification of slang: for example, the word ``tho'' is recognized by GloVe, but its representation is nowhere close to ``though'')

\subsection*{Final Model}
\label{finalmodel}

The model we ultimately present here has the following architecture:

\begin{itemize}
\item After preprocessing, the input is a length-104 sequence of 100-dimensional GloVe vectors
\item Convolution 1: 256 filters of width 7, followed by ReLU
\item Convolution 2: 128 filters of width 3, followed by ReLU
\item Max pooling: width of 2, stride of 2 (nonoverlapping)
\item Fully connected layer, followed by ReLU
\item Linear output layer: from 128 to five classes, followed by a log soft max, in preparation for an NLL criterion.
\end{itemize}

Preprocessing consists of splitting the text on whitespace, removing punctuation, and reading in up to 104 of those ``words'', ignoring and bypassing any words that are not found in the GloVe table.

We trained our final model using the following training:
\begin{itemize}
\item Learning Rate: Decreased manually every five epochs: 0.05, then 0.01, then 0.001
\item Momentum: 0.1, then 0.1, then 0.2
\item Loss function: negative log likelihood
\item Regularization: one dropout layer of 0.5 before the fully connected layer
\item Training dataset of 100,000 samples per class
\item Testing set of 1,000 samples per class
\end{itemize}

\section*{Results}

Ultimately, our model performed with TODO error on TODO.

\section*{References}

Boureau, Y. L., Ponce, J., \& LeCun, Y. (2010). A theoretical analysis of feature pooling in visual recognition. In \emph{Proceedings of the 27th International Conference on Machine Learning (ICML-10)} (pp. 111-118).

Collobert, Ronan, et al. "Natural language processing (almost) from scratch." The Journal of Machine Learning Research 12 (2011): 2493-2537.

LeCun, Y. A., Bottou, L., Orr, G. B., \& Müller, K. R. (2012). Efficient backprop. In \emph{Neural networks: Tricks of the trade} (pp. 9-48). Springer Berlin Heidelberg.

Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global vectors for word representation." Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12 (2014).

Zhang, Xiang, and Yann LeCun. "Text Understanding from Scratch." arXiv preprint arXiv:1502.01710 (2015).

\end{document}
