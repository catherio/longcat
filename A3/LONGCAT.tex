\documentclass{article}

\newenvironment{itemizedense}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{DS-GA-1008 Assignment 3}
\author{LongCat: Catherine Olsson and Long Sha and Kevin Brown}

\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{fullpage}

\begin{document} \maketitle

\begin{abstract}
This is an abstract. TODO finish me!
\end{abstract}

\section*{Part 1: Log-exponential pooling}

For part 1 of the assignment, we tuned the hyperparameters for the baseline model after implementing log-exponential pooling. With the parameters reported below, we achieved at best 51.6\% error with 50 training epochs, using 10,000 training documents per class, and 1,000 validation documents per class. 

\subsection*{Architecture and training}
We mainly focused on the following hyperparameters: dimensionality of GloVe representation for each word (inputDim), number of filters for temporal convolution (nfilter), convolutional filter size (filtsize), pooling size (poolsize), the stride for convolution filters and pooling (stride). We eventually settled on the following hyperparameters:

\begin{enumerate}
\item GLoVe vector size: 200
\item number of filters: 60
\item filter size: 10
\item pooling size: 7
\item stride: 1
\end{enumerate}

We used stochastic gradient descent to train our baseline model. We apply the learning rate at 0.1, with a momentum of 0.01. For every 10 training epochs, we reduce the learning rate by half. The batch size we apply is 128. In general, we found that increasing the dimensionality of input data helps with reducing error, i.e. increasing GloVe representation and number of convolutional filters. 

\subsection*{Implementation of log-exponential pooling} 

We also coded a Module \texttt{TemporalLogExpPooling} that performs log-exponential pooling (Boureau et al. 2010). The log-exponential pooling follows equation \ref{logexp}:

\begin{align}
u_j = \frac{1}{\beta} \log{\frac{1}{N} \sum\limits_{i=1}^{N} e^{\beta x_i}}
\label{logexp}
\end{align}

where $u$ is the module output, $x$ is the module input, $N$ is the size of pooling window, $\beta$ is a free parameter between 0 and $\inf$. When  gets close to 0, the pooling behaves more similar to an average-pooling module, whereas close to $\inf$ makes it a max-pooling module. The log-exponential pooling is a transition between average and max-pooling. We also tuned the  parameter, and found that  in the range between 10 and 100 works best. 

We used equation \ref{logexp} to compute the output $u$ based on input $x$. Each output entry is computed within a pooling window from $x$. The computation of module output is coded in function \texttt{TemporalLogExpPooling:updateOutput()}. 

Next, we aim to compute the gradient of loss with respect to the input. Using the chain rule and principles in back propagation (LeCun et al., 1998), we compute:

\begin{align}
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial u} \frac{\partial u}{\partial x}
\label{chainrule}
\end{align}

$\frac{\partial L}{\partial u}$ represents the impact that this model's output has on the loss, given the current input. It was computed by backpropagating through the downstream modules, as \texttt{gradOutput}. The other component is the relationship between the model's own inputs and outputs, $\frac{\partial u}{\partial x}$. We obtain this by differentiating equation \ref{logexp} with respect to entries in $x$. We therefore derive equation \ref{backprop}:

\begin{align}
\frac{\partial u_j}{\partial x_i} = \frac{e^{\beta x_i}}{\sum\limits_{i=1}^{N} e^{\beta x_i}}
\label{backprop}
\end{align}

We use equation \ref{backprop} to update the gradient of loss with respect to input within a pooling window. The computation of gradient input is coded in function \texttt{TemporalLogExpPooling:updateGradInput()}. Together, these functions allow the module to work with forward() and backward() functions for model prediction and model training. 

We also run a gradient checker to ensure that the module is correct. To do so, we perturb the input by a small variation in one $x_i$ entry at a time. We forward the new input through the module to compute a new output. We compute the difference between the old and new outputs, and backpropagate this difference through the module. We check if the gradient we compute equals the perturbation we first injected. 

One caveat in our current module implementation is that we use for-loops to loop over all batches and pooling windows. This regime results in slow computation during model training, and the GPU could not efficiently accelerate the computation. We think that applying matrix multiplication for the pooling operation would speed up the module computation; this could be achieved by constructing the Jacobian matrix and multiplying through the entire vector at once. We would like to work on speeding up the module computation in the future. 

\section*{Part 2: Convolutional model}

We chose to implement a convolutional model on top of pre-trained GloVe vectors as our extension for Part 2. Convolutional models improve on the bag-of-words model in the baseline, because they can incorporate word order. We chose \emph{not} to implement a recurrent model only because of the additional implementation difficulty; recurrent models can better capture long-range information.

The ``Experimentation Phase'' section below describes the experiments we performed before settling on our final model, and what motivated our choices. The ``Final Model'' section explains the final model that is behind our submission.

\subsection*{Experimentation phase}

\subsubsection*{Choosing a model}

The first model we built was a large a multi-layer network, similar to layer 2 and beyond from Zhang's paper, but made smaller for easier testing. In our first (overly ambitious) model, layer 1 was a 7-width convolutional layer with 128 filters, operating over GloVe vectors, followed by a size-3 pooling; Layers 2 through 4 were 128-filter convolutional layers (two without pooling, one with); and the final layer was fully-connected.

However, we found this many-layer network confusing to train. We could not tell what was causing difficulties -- insufficient data? too large learning rate? too much or too little momentum? insufficient regularization?

Therefore, we decided to scale back to working with a much smaller network where we could build more intuition. Although our final performance with the small network was ultimately nowhere near as impressive as it might have been if we had successfully run a more ambitious model, we ultimately learned more about the practicalities of training neural networks by working in a much smaller sandbox where we could get rapid feedback on our experiments.

Our final model, described in detail in section~\ref{finalmodel}, contains just two convolutional layers, followed by one pooling layer, before the fully-connected layer.

\subsubsection*{Architectural and training decisions}

Here are a few of the architectural decisions we considered, and how we selected our final parameters:
\begin{enumerate}
\item{\textbf{Dimensionality of GloVe vectors:}} We performed most of our testing with the smallest GloVe vectors, length 50. For final tuning, we faced a memory tradeoff between dimensionality and number of training samples. We increased the size of the GloVe vectors to 100, which still enabled us to load 100,000 training samples. Higher-dimensional vectors would have negatively impacted the size of the training set; we hypothesized that training set richness was more important than input dimensionality in making this choice.
\item{\textbf{Size convolution filters:}} How many words should the first convolutional filters span? How far should the second convolutional filters reach? We chose a size of 7 for the first layer, and 3 for the second layer, as this matched the parameterization in Zhang 2015 layers 2 and 3 (assuming that Zhang layer 1 stands for the transformation from characters to words and is thus subsumed by our GloVe input). A seven-word ``memory'' also sounded intuitively sufficient as a width for the first layer, to capture semantially interesting patterns containing sentiment information.
\item{\textbf{Amount of pooling:}} We chose not to implement pooling after the first layer, because we 
\item{\textbf{Fan-out and fan-in of convolutional layers:}} Initially we set both convolutional layers to 128 filters. Increasing the fan-out of the first layer, to 256, improved our performance from 61\% error to 57\% error at the time it was implemented. We chose not to increase the size of the second layer at the time, because we did not want to increase the number of parameters too drastically; increasing the sizes of other layers would be a promising future direction.
\item{}
\end{enumerate}

How many filters to fan out to?

Depth / number of layers?
Dimensionality of first layer?

Training decisions:
What learning rate?
What momentum?
What learning rate decay, or what schedule of periodic learning rate cuts?
What initialization? (We did not depart from the Torch initialization at all)
How much data?
-> many of our memory woes were actually due to too much *testing* data
-> still, we there was a tradeoff between GloVe length and number of samples; we chose more samples, not more GloVe length! this was huge!

Chose to use a very small model, and quite little data, and the smallest Glove vectors, to hone in on even the right ballpark for training parameters.

However, once we included more data, the early plateauing and oscillation went away spontaneously. This seems to indicate that perhaps some of our ``stuckness'' was due to not enough data, rather than poor training parameters.

We did not use any data augmentation; Zhang's paper suggested that this was not necessary.

\subsubsection*{Text processing choices}
How many words to keep / how to handle variable-length?
How many dimensions?
Padding at start and end?
Handling numbers?
Punctuation?
Capital letters? (reference the SENNA paper here)
``tho'' is in the dictionary, but isn't close to ``though''

\subsection*{Final Model: Architecture and Training}
\label{finalmodel}

The model we ultimately present here has the following architecture:

We used the following parameters and learning techniques to train it:
\begin{itemize}
\item Learning Rate: TODO
\item Momentum: TODO
\item Loss function: negative log likelihood (nll)
\item Dropout: 
\item TODO train/validation
\end{itemize}

\section*{Results}


\section*{References}

Boureau, Y. L., Ponce, J., \& LeCun, Y. (2010). A theoretical analysis of feature pooling in visual recognition. In \emph{Proceedings of the 27th International Conference on Machine Learning (ICML-10)} (pp. 111-118).

LeCun, Y. A., Bottou, L., Orr, G. B., \& MÃ¼ller, K. R. (2012). Efficient backprop. In \emph{Neural networks: Tricks of the trade} (pp. 9-48). Springer Berlin Heidelberg.

Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global vectors for word representation." Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12 (2014).

Zhang, Xiang, and Yann LeCun. "Text Understanding from Scratch." arXiv preprint arXiv:1502.01710 (2015).

\end{document}
