\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}

\newenvironment{itemizedense}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{DS-GA-1008 Assignment 2: Team LongCat}

\author{
Catherine Olsson, Long Sha, and Kevin Brown \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Overall, we implemented two main strategies: one basic supervised-learning
strategy, and one unsupervised-feature-learning strategy based on Dosovitskiy
et al. TODO REFERENCE. TODO what performance was achieved with each?
\end{abstract}

\section{Supervised-only network}

\subsection{Architecture}
TODO update this!

The input size is 3 color channels, and 96-by-96 pixels. 

TODO remaining layers

\subsection{Training}

TODO verify that the following is true!
We preprocessed the images by subtracting the global mean for each channel, and dividing by the standard deviation (zscore), for the train data. We then used this mean and standard deviation to transform the test data (to keep training and testing data separate).

No data augmentations were used. We used a dropout probability of 0.5 on the third layer. We used stochastic gradient descent with the following parameter values:

TODO check that the following are right

\begin{itemizedense}
\item Learning Rate $1 \times 10^{-7}$
\item Momentum of $0$
\item Weight decay of $0$
\item Loss function: negative log likelihood (nll)
\item TODO train/validation split
\end{itemizedense}

\subsection{Results}

TODO submit results!

\section{Unsupervised feature-learning network}

\subsection{Overview of approach}

The unsupervised feature-learning approach proceeds in two steps. The first step is to learn a feature representation from the unlabeled data. This is achieved by generating ``surrogate classes'', each consisting of a single image patch with various distortions applied, and then training a network to discriminate these surrogate classes. The second step is to train a classifier on the true labeled data, after first transforming the labeled images to the new feature representation.

\subsection{Step 1: Unsupervised feature learning}

\subsubsection{Surrogate data generation}

A surrogate dataset is generated by transforming patches of images drawn from the unlabeled image set. For each of the $n$ surrogate classes, a random image is chosen, a candidate 32x32 seed patch is selected, and $m$ exemplar patches are generated by applying random transformations to the original image.

First, candidate seed patches were screened for having enough variation in intensity, to avoid selecting uniform feature-less patches. We achieved this by filtering each candidate patch with a simple horizontal and vertical gradient filter (namely ${{1, 0, -1}}$ and ${{1},{0},{-1}}$). We set a minimum threshold for the overall sum of the absolute values of the pixel intensities in the two filtered images. Candidate patches which did not surpass this threshold were rejected. We found that a setting of $600$ produced interesting patches without taking too long to complete the search.

Second, we generated $m$ transformed patches by randomly applying the following transformations:

\begin{itemizedense}
\item Rotation: up to 20 degrees, i.e. 0.35 radians.
\item Contrast: Raise the S and V components to a power between 0.25 and 3; multiply by 0.7 through 1.4; add -0.1 to 0.1.
\item Hue: add a value between -0.1 and 0.1
\item Translation: within 0.2 of the patch size
\end{itemizedense}

We did not implement the PCA-based contrast transformation described in Dostovitskiy et al, because it was more complicated to implement. We also removed the scaling transformation described in Dostovitskiy et al. simply because it was buggy and causing problems, and we did not have time to fix it before submission. We believe our results would have been more robust and our filters more scale-invariant if we had succeeded at implementing the scale transformation.

We experimented with using surrogate datasets with TODO RANGE N surrogate classes, each with TODO RANGE M exemplars. TODO what did we find?

TODO figure here of image patches

\subsubsection{Feature representation training}
The input of the feature representation training step is a surrogate dataset of $n$ generated classes, each with $m$ images (32x32) derived from each class' seed patch.

Preprocessing TODO. Architecture TODO. 

We used a dropout probability of 0.5 on the third layer. We used standard stochastic gradient descent, with the following parameter values:

\begin{itemizedense}
\item Learning Rate $1 \times 10^{-3}$
\item Momentum of $0$
\item Weight decay of $0$
\item Loss function: negative log likelihood (nll)
\item Training set is 75\% of the data; validation set is a random 25\%
\end{itemizedense}

We implemented the ability to perform ten-fold cross-validation, but did not use it much for model selection because it proved to be very slow.

\subsection{Step 2: Supervised classification}

\subsubsection{Feature representation transformation}

TODO explain how to use the 32x32 module to transform to a new feature representation

\subsubsection{Supervised training}

Preprocessing TODO. Architecture TODO.

We used a dropout probability of 0.5 on the third layer. We used standard stochastic gradient descent, with the following parameter values:

\begin{itemizedense}
\item Learning Rate $1 \times 10^{-3}$
\item Momentum of $0$
\item Weight decay of $0$
\item Loss function: negative log likelihood (nll)
\item Training set is 75\% of the data; validation set is a random 25\%
\end{itemizedense}

\subsection{Results}

TODO fill in results!

Any conditions/variations we tried? How did they do?

\end{document}
