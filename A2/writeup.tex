\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}

\newenvironment{itemizedense}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}




%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Formatting Instructions for NIPS 2014}


\author{
Catherine Olsson, Long Sha, and Kevin Brown\thanks{all} \\
Center for Neural Science\\
New York University\\
\texttt{catherio@nyu.edu} \\
\And
Long Sha \\
CNS NYU \\
\texttt{ls3470@nyu.edu} \\
\AND
Kevin Brown \\
CNS NYU \\
\texttt{kab695@nyu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Overall, we implemented two main strategies: one basic supervised-learning
strategy, and one unsupervised-feature-learning strategy based on Dosovitskiy
et al. TODO REFERENCE. TODO what performance was achieved with each?
\end{abstract}

\section{Supervised-only network}

\subsection{Preprocessing}

TODO totally unchanged, fix this!

We preprocessed each image in the following manner. First, we converted the
images from RGB representations to YUV, to separate luminance from color, with
the working assumption that pixels that are part of address numbers will have
different luminance, but possibly similar color. (Note: because torch's rgb2yuv
requires floating point representation, we recast the original representation of the data.

Next, we normalized each feature globally, by subtracting the global mean for
each channel, and dividing by the standard deviation (zscore), for the train
data. We then used this mean and standard deviation to transform the test data
(to keep training and testing data separate).



\subsection{Architecture}

The input size is 3 color channels, and 96-by-96 pixels. 

TODO remaining layers

\subsection{Learning Techniques and Training Procedure}
No data augmentations were used beyond the standard preprocessing. We used a dropout probability of 0.5 on the third layer.

We used standard stochastic gradient descent, since the optimization problem is
non-convex for our architecture. We used the following parameter values:

TODO check that the following are right

\begin{itemize}
\item Learning Rate $1 \times 10^{-7}$
\item Momentum of $0$
\item Loss function: negative log likelihood (nll)
\item TODO train/validation
\end{itemize}

\subsection{Results}

\section{Unsupervised feature-learning network}

\subsection{Overview of approach}

The unsupervised feature-learning approach proceeds in two steps. The first step is to learn a feature representation from the unlabeled data. This is achieved by generating ``surrogate classes'', each consisting of a single image patch with various distortions applied, and training a network to discriminate these surrogate  classes. The second step is to train a classifier on the true labeled data, after first transforming the labeled images to the new feature representation.

\subsection{Step 1: Unsupervised feature learning}

\subsubsection{Surrogate data generation}

A surrogate dataset is generated by transforming patches of images drawn from the unlabeled image set. For each of the $nClasses$ surrogate classes, a random image is chosen, a candidate 32x32 seed patch is selected, and $nExemplars$ exemplar patches are generated by applying random transformations to the original image.

First, candidate seed patches were screened for having enough variation in intensity, to avoid selecting uniform feature-less patches. We achieved this by filtering each candidate patch with a simple horizontal and vertical gradient filter (namely ${{1, 0, -1}}$ and ${{1},{0},{-1}}$). We set a minimum threshold for the overall sum of the absolute values of the pixel intensities in the two filtered images. Candidate patches which did not surpass this threshold were rejected. We found that a setting of $600$ produced interesting patches without taking too long to complete the search.

Second, we generated $nExemplars$ transformed patches by randomly applying the following transformations:

\begin{itemize}
\item Rotation: up to 20 degrees, i.e. 0.35 radians.
\item Contrast: Raise the S and V components to a power between 0.25 and 3; multiply by 0.7 through 1.4; add -0.1 to 0.1.
\item Hue: add a value between -0.1 and 0.1
\item Translation: within 0.2 of the patch size
\end{itemize}

We did not implement the PCA-based contrast transformation described in Dostovitskiy et al, because it was more complicated to implement. We also removed the scaling transformation described in Dostovitskiy et al. simply because it was buggy and causing problems, and we did not have time to fix it before submission. We believe our results would have been more robust and our filters more scale-invariant if we had succeeded at implementing the scale transformation.

We experimented with using surrogate datasets with TODO RANGE N surrogate classes, each with TODO RANGE M exemplars. TODO what did we find?

\subsubsection{Feature representation training}
The input of the The output of the unsupervised feature learning stage is a network which turns 32x32 iamge patches.

TODO graphic here of image patches

\subsection{Step 2: Supervised classification}

\subsection{Preprocessing}

\subsection{Architecture}

The input size is 3 color channels, and 96-by-96 pixels. 

TODO remaining layers

\subsection{Learning Techniques and Training Procedure}
No data augmentations were used beyond the standard preprocessing. We used a dropout probability of 0.5 on the third layer.

We used standard stochastic gradient descent, since the optimization problem is
non-convex for our architecture. We used the following parameter values:

TODO check that these are right

\begin{itemize}
\item Learning Rate $1 \times 10^{-7}$
\item Momentum of $0$
\item Loss function: negative log likelihood (nll)
\item TODO train/validation
\end{itemize}

\subsection{Results}

TODO fill in results!


%% \subsection{Keywords for paper submission}
%% Your NIPS paper can be submitted with any of the following keywords (more than one keyword is possible for each paper):

%% \begin{verbatim}
%% Bioinformatics
%% Biological Vision
%% Brain Imaging and Brain Computer Interfacing
%% Clustering
%% Cognitive Science
%% Control and Reinforcement Learning
%% Dimensionality Reduction and Manifolds
%% Feature Selection
%% Gaussian Processes
%% Graphical Models
%% Hardware Technologies
%% Kernels
%% Learning Theory
%% Machine Vision
%% Margins and Boosting
%% Neural Networks
%% Neuroscience
%% Other Algorithms and Architectures
%% Other Applications
%% Semi-supervised Learning
%% Speech and Signal Processing
%% Text and Language Applications

%% \end{verbatim}
\end{document}
