\documentclass{article}
\title{DS-GA-1008 Assignment 2}
\author{LongCat: Catherine Olsson and Long Sha and Kevin Brown}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{fullpage}

\newenvironment{itemizedense}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\begin{document} \maketitle

\section*{Summary of Strategies}

Overall, we implemented two main strategies: one basic supervised-learning strategy, and one unsupervised-feature-learning strategy based on Dosovitskiy et al. TODO REFERENCE. TODO what performance was achieved with each?

\section{Supervised-only network}

\subsubsection*{Preprocessing}

TODO totally unchanged, fix this!

We preprocessed each image in the following manner. First, we converted the
images from RGB representations to YUV, to separate luminance from color, with
the working assumption that pixels that are part of address numbers will have
different luminance, but possibly similar color. (Note: because torch's rgb2yuv
requires floating point representation, we recast the original representation of the data.

Next, we normalized each feature globally, by subtracting the global mean for
each channel, and dividing by the standard deviation (zscore), for the train
data. We then used this mean and standard deviation to transform the test data
(to keep training and testing data separate).

\subsubsection*{Architecture}

The input size is 3 color channels, and 96-by-96 pixels. 

TODO remaining layers

\subsubsection*{Learning Techniques and Training Procedure}
No data augmentations were used beyond the standard preprocessing. We used a dropout probability of 0.5 on the third layer.

We used standard stochastic gradient descent, since the optimization problem is
non-convex for our architecture. We used the following parameter values:

TODO check that the following are right

\begin{itemizedense}
\item Learning Rate $1 \times 10^{-7}$
\item Momentum of $0$
\item Loss function: negative log likelihood (nll)
\item TODO train/validation
\end{itemizedense}

\subsection*{Results}

\section{Unsupervised feature-learning network}

\subsection*{Overview of approach}

The unsupervised feature-learning approach proceeds in two steps. The first step is to learn a feature representation from the unlabeled data. This is achieved by generating ``surrogate classes'', each consisting of a single image patch with various distortions applied, and training a network to discriminate these surrogate  classes. The second step is to train a classifier on the true labeled data, after first transforming the labeled images to the new feature representation.

\subsection*{Step 1: Unsupervised feature learning}

\subsubsection*{Surrogate data generation}

A surrogate dataset is generated by transforming patches of images drawn from the unlabeled image set. For each of the $nClasses$ surrogate classes, a random image is chosen, a candidate 32x32 seed patch is selected, and $nExemplars$ exemplar patches are generated by applying random transformations to the original image.

First, candidate seed patches were screened for having enough variation in intensity, to avoid selecting uniform feature-less patches. We achieved this by filtering each candidate patch with a simple horizontal and vertical gradient filter (namely ${{1, 0, -1}}$ and ${{1},{0},{-1}}$). We set a minimum threshold for the overall sum of the absolute values of the pixel intensities in the two filtered images. Candidate patches which did not surpass this threshold were rejected. We found that a setting of $600$ produced interesting patches without taking too long to complete the search.

Second, we generated $nExemplars$ transformed patches by randomly applying the following transformations:

\begin{itemizedense}
\item Rotation: up to 20 degrees, i.e. 0.35 radians.
\item Contrast: Raise the S and V components to a power between 0.25 and 3; multiply by 0.7 through 1.4; add -0.1 to 0.1.
\item Hue: add a value between -0.1 and 0.1
\item Translation: within 0.2 of the patch size
\end{itemizedense}

We did not implement the PCA-based contrast transformation described in Dostovitskiy et al, because it was more complicated to implement. We also removed the scaling transformation described in Dostovitskiy et al. simply because it was buggy and causing problems, and we did not have time to fix it before submission. We believe our results would have been more robust and our filters more scale-invariant if we had succeeded at implementing the scale transformation.

We experimented with using surrogate datasets with TODO RANGE N surrogate classes, each with TODO RANGE M exemplars. TODO what did we find?

TODO figure here of image patches

\subsubsection*{Feature representation training}
The input of the feature representation training step is a surrogate dataset of $nClasses$ generated classes, each with $nExemplars$ images (32x32) derived from each class' seed patch.

Preprocessing TODO. Architecture TODO. 

We used a dropout probability of 0.5 on the third layer. We used standard stochastic gradient descent, with the following parameter values:

\begin{itemizedense}
\item Learning Rate $1 \times 10^{-3}$
\item Momentum of $0$
\item Weight decay of $0$
\item Loss function: negative log likelihood (nll)
\item Training set is 75\% of the data; validation set is a random 25\%
\end{itemizedense}

We implemented the ability to perform ten-fold cross-validation, but did not use it much for model selection because it proved to be very slow.

\subsection*{Step 2: Supervised classification}

\subsection*{Feature representation transformation}

TODO explain how to use the 32x32 module to transform to a new feature representation

\subsection*{Supervised training}

Preprocessing TODO. Architecture TODO.

We used a dropout probability of 0.5 on the third layer. We used standard stochastic gradient descent, with the following parameter values:

\begin{itemizedense}
\item Learning Rate $1 \times 10^{-3}$
\item Momentum of $0$
\item Weight decay of $0$
\item Loss function: negative log likelihood (nll)
\item Training set is 75\% of the data; validation set is a random 25\%
\end{itemizedense}

\subsection*{Results}

TODO fill in results!

Any conditions/variations we tried? How did they do?

\end{document}
